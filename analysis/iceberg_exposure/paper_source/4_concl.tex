\section{Limitations}
The Iceberg Index captures technical exposure—where AI capabilities overlap with occupational skills—not actual displacement or labor market outcomes. Real-world impacts depend on adoption choices, firm strategies, worker adaptation, societal acceptance and policy interventions. The Index functions as a capability map enabling scenario-based planning rather than deterministic forecasting.

\subsection{Validation}
The framework achieves strong empirical validation against independent data sources: high recall rates predicting career transitions and substantial geographic agreement with observed AI adoption patterns (see Section~\ref{sec:validation} for detailed results). These results confirm the Index captures genuine workforce structure and plausible adoption patterns. However, validation anchors reflect current contexts—career transitions from human-only labor markets and technology-sector AI usage—which may not fully capture adoption dynamics in non-technology sectors. See Appendix A for validation methodology and anchor constraints.

\subsection{Measurement Scope}
The Index measures exposure from production AI systems deployed in enterprise contexts—automation platforms, copilots, and industry-specific tools—rather than frontier model capabilities tested on academic benchmarks. This focuses on immediate business impact: tools reshaping work today.
Recent evaluations like GDPval~\cite{openai2025gdpval} and APEX~\cite{vidgen2025apex} test whether frontier models can perform isolated professional tasks at human-equivalent quality. These benchmarks measure task-level capability; our framework measures systemic workforce exposure across 151 million workers. Both dimensions matter for workforce planning. A model may perform well on benchmarks yet not reshape work until packaged into deployable tools, integrated into workflows, and adopted at scale. By cataloging 13,000+ production systems, the Iceberg Index captures where AI is positioned to affect work now, providing conservative estimates of immediate exposure. Future work could examine alignment between high-exposure occupations and benchmark performance. See Appendix A for tool cataloging methodology.

\subsection{Modeling Choices}
Key assumptions affecting interpretation:
\begin{itemize}
\item \textbf{Wage-value weighting:} Enables consistent comparison across occupations but smooths over job quality dimensions (stability, autonomy, career progression) and masks within-occupation heterogeneity where some tasks face high automation potential while others remain human-intensive.
\item \textbf{Skill transferability:} The Index assumes skills demonstrated in one occupational context transfer to others, establishing an upper bound on exposure. This enables scenario analysis where transferability can be varied, but may overestimate near-term exposure if domain-specific adaptation proves more challenging than assumed.
\item \textbf{Digital AI focus:} Analysis covers cognitive and administrative automation where deployment is observable. Physical robotics excluded due to immature adoption data; see Appendix C for scope rationale.
\end{itemize}
See Appendices A and C for detailed data sources, implementation choices, and computational infrastructure.

\subsection{Causal Interpretation}
Validation is correlational rather than causal. Agreement with observed adoption patterns increases confidence but does not prove exposure drives outcomes. External factors—state investment, infrastructure, regulation—mediate how capability translates to impact. This limitation is inherent to forward-looking analysis: policymakers cannot wait for causal evidence of disruption before preparing responses. The Index provides the best available prospective indicator, validated against current patterns.

\subsection{Complementary Role}
The Index complements traditional labor metrics (GDP, unemployment, wages) rather than replacing them. Traditional metrics track realized outcomes; the Iceberg Index reveals potential exposure before adoption crystallizes. Together they enable comprehensive workforce planning combining retrospective assessment and prospective intelligence.

\section{Conclusion}
The Iceberg Index measures where AI technical capabilities overlap with human occupational skills across 151 million workers, providing forward-looking intelligence to complement traditional workforce metrics that track employment outcomes after disruption occurs.

The analysis reveals a substantial measurement gap. Current AI adoption concentrates in technology occupations representing 2.2\% of labor market wage value. Yet AI technical capability extends to cognitive and administrative tasks spanning 11.7\% of the labor market—approximately \$1.2 trillion in wage value across finance, healthcare, and professional services. This fivefold exposure difference is geographically distributed nationwide rather than concentrated in coastal hubs, indicating that workforce preparation strategies based on visible technology-sector signals may substantially undercount transformation potential. Validation against independent data confirms the framework captures plausible workforce patterns.

The Index measures technical exposure—where AI can perform occupational tasks—not displacement outcomes. Actual workforce impacts depend on firm strategies, worker adaptation, and policy choices. By simulating how capabilities might spread under different scenarios, Project Iceberg enables states to test interventions before committing resources, transforming workforce planning from reactive crisis management to strategic foresight. Future work will model adoption dynamics, extend to physical automation as robotics mature, and integrate task-level quality benchmarks.

The Iceberg Index provides measurable intelligence for critical workforce decisions: where to invest in training, which skills to prioritize, how to balance infrastructure with human capital. It reveals not only visible disruption in technology sectors but the larger transformation beneath the surface. By measuring exposure before adoption reshapes work, the Index enables states to prepare rather than react—turning AI into a navigable transition.

\section{Acknowledgement}
We thank Oak Ridge National Lab compute award for the simulation infrastructure; Anthropic for providing API credits; and MIT MLC for funding Ayush Chopra.

% \section{Limitations}

% Validation provides partial reassurance but is not comprehensive. Our skill-based embeddings achieve 85\% recall in predicting common career transitions, and our exposure predictions align with Anthropic’s Economic Index at 69\% perfect state categorization and 87\% directional agreement. These results confirm that skill-based structure and adoption patterns are plausibly captured. However, both anchors are limited: career transitions reflect historical human-only labor markets, and AEI adoption data captures only early-adopter knowledge workers in technology-intensive occupations.

% A second limitation concerns quality thresholds. The Index measures whether AI systems can technically execute tasks, not whether they perform them at human-equivalent quality levels. Firms adopt tools only when outputs cross domain-specific quality thresholds that vary by application. Recent evaluation suites such as GDPval~\cite{openai2025gdpval} and APEX~\cite{vidgen2025apex} directly address this quality dimension by testing whether frontier models can perform professional tasks at or above human level. Our framework is orthogonal: it captures systemic exposure across 151M workers, not competence on individual deliverables. Future work could examine whether high-exposure occupations also align with stronger performance on task-level benchmarks.

% Modeling assumptions also constrain interpretation. Simulation dynamics follow historical technology diffusion patterns, which may not capture AI’s unique deployment curve. Wage-value weighting enables consistent comparison across occupations and regions, but smooths over job quality dimensions such as stability or autonomy, and masks heterogeneity within occupations where some tasks are automatable and others remain human-intensive. Mapping 13,000+ AI tools to occupational skills required subjective judgments despite consistency checks. The current analysis focuses on digital AI tools where deployment patterns are observable, while physical robotics remains excluded due to immature adoption data.

% Finally, our validation is correlational rather than causal. Agreement with AEI increases confidence that Iceberg exposure reflects plausible adoption pathways, but correlation alone does not prove that exposure drives adoption. External factors such as state-level investment, firm concentration, or infrastructure readiness may mediate outcomes. The Iceberg Index should therefore be used as a complementary planning instrument alongside traditional labor metrics, providing directional foresight rather than deterministic prediction.


% \section{How to use Project Iceberg}
% Project Iceberg provides a policy sandbox for exploring workforce scenarios in the AI economy. The Iceberg Index measures technical exposure—the overlap between AI capabilities and human work—weighted by wage value across occupations, industries, and regions. The platform supports baseline assessment to identify where automation potential is concentrated, scenario testing to model how different technology maturity levels, adoption rates, or policy interventions affect workforce composition, comparative analysis to benchmark exposure patterns across states, and investment targeting to direct resources toward high-exposure occupations with strong reskilling potential. Effective application requires integration with state-specific intelligence from labor departments and economic development agencies, which provide visibility into regional industry dynamics, employer adoption patterns, infrastructure constraints, and competitive positioning.

% The Iceberg Index complements traditional workforce metrics by highlighting where technical vulnerabilities may concentrate. States can establish baseline measurements and test policy scenarios, recognizing that actual impacts depend on adoption choices and effective strategy requires integration with local labor market intelligence. Tracking changes over time provides maximum value: periodic reassessment reveals whether exposure is concentrating or dispersing, which occupations face accelerating risk, and whether interventions are reducing vulnerability.


% ===============================

% \section{Limitations}
% The Iceberg Index captures technical exposure—the share of skills that AI systems could perform—not actual displacement or labor market outcomes. Real-world impacts depend on adoption choices, firm strategies, worker adaptation, and policy interventions outside the model's scope. Policymakers should treat the Index as a scenario-based exposure indicator rather than a forecast of job loss.

% The framework's skill-based representations are validated against empirical career transition data (85\% recall in predicting common career moves) and current AI adoption patterns via the Anthropic Economic Index (69\% perfect state categorization). However, validation reflects early-adopter knowledge workers in technology-intensive occupations and may not fully represent broader industries or geographies. The Index measures whether AI systems can technically execute tasks, not whether they perform them at human-equivalent quality levels—firms adopt tools when capabilities meet their context-specific quality thresholds, which vary by application.

% Simulation assumptions follow historical technology diffusion patterns, which may not capture AI's unique deployment dynamics. The framework measures exposure through wage values, providing consistent comparison across occupations and regions, but cannot capture job quality dimensions such as stability or autonomy, nor distributional effects across demographic groups. Methodological choices—including mapping 13,000+ AI tools to occupational skills—required subjective judgments despite efforts to ensure consistency. The analysis focuses on digital AI tools where deployment patterns are observable, while physical robotics remains outside scope due to less mature adoption data.

% Finally, the framework measures automation potential based on emerging AI capabilities rather than predicting transformation timelines. It provides no estimates of broader economic feedback loops such as productivity gains or new job creation. Effective application requires integration with state-level labor market intelligence and local conditions. The Index should be used as a complementary tool alongside traditional labor metrics, providing directional foresight about technical capability overlap rather than deterministic prediction.



% Project Iceberg provides a policy sandbox for exploring workforce scenarios in the AI economy, enabling states to test different assumptions about technology adoption, investment strategies, and regulatory approaches before committing resources. The Iceberg Index serves as a skills-centered metric for tracking how human-AI collaboration evolves across occupations, industries, and regions. Rather than providing prescriptive recommendations, the platform offers analytical capabilities that allow policymakers to evaluate multiple pathways and their potential consequences under various conditions.

% The Iceberg Index should be used as a complementary tool alongside traditional workforce metrics. It highlights where technical vulnerabilities may concentrate, but actual impacts depend on adoption choices and policy interventions that remain in human control. States can establish baseline measurements and use scenario testing to explore potential effects of proposed policies, recognizing that effective workforce strategy requires multiple analytical perspectives and integration with local labor market intelligence.

% \subsection{Limitations and Assumptions}
% The Iceberg Index measures technical exposure—the share of skills that AI systems could perform—not actual displacement or labor market outcomes. Real-world impacts depend on adoption choices, firm strategies, worker adaptation, and policy interventions outside the model's scope. Several key limitations and assumptions shape how the framework should be applied.

% \section{How to use Project Iceberg}
% Project Iceberg provides a policy sandbox for exploring workforce scenarios in the AI economy, enabling states to test different assumptions about technology adoption, investment strategies, and regulatory approaches before committing resources. The Iceberg Index serves as a skills-centered metric for tracking how human-AI collaboration evolves across occupations, industries, and regions. Rather than providing prescriptive recommendations, the platform offers analytical capabilities that allow policymakers to evaluate multiple pathways and their potential consequences under various conditions.

% The Iceberg Index should be used as a complementary exposure indicator, much like a risk model or epidemiological forecast: it highlights where vulnerabilities may arise, but actual impacts depend on social, economic, and policy choices that remain in human control.

% Several key assumptions shape how the platform should be applied. Scenario modeling relies on historical technology diffusion patterns, but AI adoption may follow different trajectories due to policy interventions, regulatory changes, or economic disruptions that cannot be fully anticipated. The platform measures workforce exposure through wage values but cannot model broader economic feedback loops such as productivity gains, new job creation, or GDP impacts from AI deployment. The current analysis focuses on digital AI tools where deployment patterns are observable, while physical robotics and manufacturing automation remain outside scope. Analysis depends on publicly available occupational data that may lag emerging roles or rapid skill evolution, and effective application requires integration with state-level labor market intelligence and local economic conditions that only state agencies possess. The Iceberg Index measures automation potential based on emerging AI capabilities rather than predicting transformation timelines, enabling policymakers to assess the scope of change while determining their own intervention strategies and adoption pace.

% Policymakers can treat the Iceberg Index as a complementary indicator alongside traditional workforce metrics like employment rates and wage growth. Tracking changes by occupation, industry, and region over time provides signals about workforce transformation patterns, though interpretation should account for the limitations noted above. States may find value in establishing baseline measurements and using scenario testing to explore potential effects of proposed policies before implementation, while recognizing that the platform provides one perspective among many needed for comprehensive workforce planning.


% \PB{
% \subsection{Limitations}

% While the Iceberg Index provides a novel skills-centered measure of workforce exposure to AI, several limitations should be acknowledged to guide interpretation. First, the Index captures technical exposure, the share of skills that AI systems could perform, not actual displacement or labor-market outcomes. Real-world impacts depend on adoption choices, firm strategies, worker adaptation, and policy interventions. Policymakers should therefore treat the Index as a scenario-based exposure indicator, not a forecast of job loss.

% Second, the current validation relies primarily on adoption signals from the Anthropic Economic Index, which reflects early-adopter knowledge workers and may not represent broader industries or geographies. Although the framework aligns well with leading and lagging states, further validation using complementary datasets (e.g., job postings, occupational transitions, LinkedIn skill trends) is needed to strengthen robustness. Similarly, our simulation assumptions about adoption follow historical diffusion curves, which may not fully capture the unique dynamics of AI uptake.

% Third, the framework currently emphasizes wage value as the main measure of workforce exposure. While wages provide a consistent and comparable metric, they cannot capture broader aspects of job quality such as stability, security, or worker well-being, nor do they account for inequality across demographic groups. Incorporating complementary indicators remains an important direction for future work.

% Finally, methodological choices including the mapping of over 13,000 AI tools to occupational skills, introduce some degree of subjectivity. While steps were taken to ensure consistency, more transparent coding protocols and external validation would improve reproducibility. Taken together, these limitations highlight that the Iceberg Index should be used as a complementary tool alongside traditional labor metrics and qualitative insights, providing directional foresight rather than deterministic prediction.

% \subsection{Limitations for Policymakers}

% The Iceberg Index highlights exposure, skills that AI could technically perform, rather than predicting actual job losses. Real impacts will depend on adoption choices, firm strategies, and policy responses. Current validation is strongest in knowledge work and may not fully capture all industries. Results should be interpreted as scenarios of potential change, not forecasts, and used alongside traditional economic indicators when planning investments and workforce strategies. % }