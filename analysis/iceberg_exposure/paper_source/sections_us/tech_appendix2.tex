\section*{Appendix A: Data Sources and Framework}
The Iceberg Index combines four key components: skills required by occupations, automatability of those skills by AI systems, likelihood of adoption across industries, and economic value of affected work. Our framework integrates multiple data sources to model each component.

\paragraph{Skills Required by Occupations}: The O*NET Database~\cite{onet2024} provides both quantitative and qualitative skill requirements across 923 occupations. Numerical ratings cover Work Activities (41 behaviors like "Analyzing Data," "Interacting with Computers"), Skills (35 capacities like "Programming," "Critical Thinking"), and Knowledge areas (33 domains like "Economics and Accounting," "Engineering and Technology") with importance ratings (1-5 scale) and level ratings (0-7 scale) based on expert analyst assessments and incumbent worker surveys. Rich textual descriptions capture daily work activities, work values, cognitive/psychomotor/technical requirements, and detailed task descriptions that reveal what workers actually do beyond numerical scores. Categorical classifications include work context factors, education requirements, and physical demands. This multi-modal approach captures both measurable skill requirements and qualitative aspects of work that together define what each occupation entails.

\paragraph{AI System Capabilities:} We catalog over 13,000 production-ready AI tools from Model Context Protocol implementations (software development tools), the Zapier automation platform (workflow systems), and the OpenTools directory (specialized applications). These represent AI capabilities that can be packaged into deployable systems for specific occupational contexts, rather than raw frontier model performance on academic benchmarks. To align these tools with the Bureau of Labor Statistics (BLS) skill taxonomy, we develop a semi-automated mapping pipeline. Specifically, we use in-context learning with large language models to infer which skills each tool can perform, based on its task descriptions and metadata. For calibration, we split 600 occupations as training prompts and reserve 300 occupations for validation, enabling the model to learn skill-task correspondences from human-labeled BLS tasks. The model then predicts skill coverage for the tool set, which is manually reviewed to ensure consistency. This hybrid approach allows us to generate skill capability profiles for AI tools at scale, while retaining human oversight to correct systematic errors and validate uncertain cases. The result is a skill-level capability matrix that enables direct comparison between human job requirements and AI system functionality across the same dimensions.

\paragraph{Economic Value of Work}: The Bureau of Labor Statistics Occupational Employment and Wage Statistics~\cite{bls2024} provides employment counts and median wages for all 923 occupations across states and metropolitan areas, covering approximately 134 million wage and salary workers. The American Community Survey~\cite{acs2025} contributes geographic and demographic distributions for 151 million workers across 3,000 counties, including age, education, industry context, and commuting patterns. This enables calculation of total wage value potentially affected by automation and supports aggregation from individual workers to state and national levels.

% \paragraph{Validation Anchors and Their Constraints:}
% Career transition data from O*NET reflects worker mobility patterns  in human-only labor markets. While this validates that skill-based representations capture genuine occupational relationships, it may not fully reflect how AI reshapes career pathways by automating entry-level tasks or creating new hybrid roles.

% The Anthropic Economic Index measures AI usage among millions of Claude users, with concentration in computing and technical occupations. This provides strong validation for technology-sector exposure but limited insight into adoption patterns in administrative, financial, and service occupations where technology usage is less intensive. Future validation should incorporate broader adoption data as AI deployment expands beyond current technology concentrations.

% \paragraph{Adoption Likelihood Patterns}: Current adoption behavior derives from the Anthropic Economic Index~\cite{appelmccrorytamkin2025geoapi}, measuring real-world AI usage patterns from millions of Claude users across states and occupations, with particular concentration in computing and technical roles. This provides validation for present-day adoption in technology-intensive work and establishes baseline adoption rates for computing tools. Historical technology adoption patterns come from O*NET's Tools and Technology Database, documenting over 20,000 technology tools used across occupations based on incumbent worker surveys. We  technology intensity ratios $TI_i$ = (total\_tools\_i / max\_tools\_across\_occupations) and measure translation across different industries and time periods.

% \section*{Appendix B: Validation Framework}
% We validate our skills-based framework through two sequential tests that establish both structural validity and predictive accuracy. The validation covers components where empirical verification is possible, while acknowledging that forward projections rely on methodological assumptions rather than empirical validation.

% \paragraph{Job Similarity Validation}: We test whether occupations with similar skill profiles in our framework exhibit similar characteristics in empirical workforce data. Using our skill-based embeddings, we calculate cosine similarity scores between all occupation pairs and compare these against career transition networks extracted from O*NET's career exploration and job transition data~\cite{onet2024}. Career transition data captures empirically observed mobility patterns where workers commonly move between occupations, providing ground truth for occupational relationships. We identify the top percentile of similar occupation pairs according to our skill embeddings and test whether these correspond to frequent career transitions in the empirical data. Our embeddings achieve 85\% recall in predicting these transition relationships, meaning that 85\% of commonly observed career moves involve occupations that our skill-based framework identifies as highly similar. This confirms that skill-based representations capture genuine labor market structure rather than theoretical constructs, providing validation for using the same framework to assess AI-human skill overlap.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.97\linewidth]{sections_us/images_us/iceberg_validation.png}
%     \caption{a) We validate Job Embeddings by predicting similarity edges from BLS Surveys. b) We use the embedding to predict an automation susceptibility score for each job and calibrate to a 1-10 score. This is binned as Very Low, Low, Medium, High. c) We map the susceptibility of jobs to percentage of the labor market. Around 25\% employees work in jobs with high or very high base automation susceptibility in the next few years. We use this data as input to the iceberg index computation will couples with real-time AI maturing and adoption behavior data to estimate risk scores.}
%     \label{fig:risk-intensity-score}
% \end{figure}

% \paragraph{Current Exposure Validation}: Building on validated skill representations, we test whether automation exposure predictions align with actual AI usage patterns in sectors where visible impacts are occurring. We compare Surface Index state rankings against the Anthropic Economic Index (AEI) [4], which measures real-world AI usage patterns from millions of Claude users across states and industries. The AEI provides independent ground truth for current AI adoption, with industry analysis showing concentration in computing and technical occupations where our Surface Index focuses. Comparing state rankings across three balanced adoption categories (leading, middle, lagging states), we find 69\% perfect agreement between predicted exposure and observed usage patterns. Strong consensus emerges in extreme categories: 8 of 13 leading states and 9 of 13 lagging states show perfect agreement. Disagreements concentrate among middle-tier performers where category boundaries are less distinct and likely reflect differences between individual consumer usage patterns (captured by AEI) and structural workforce exposure (measured by Surface Index). This validation demonstrates that exposure predictions reflect genuine adoption behavior rather than theoretical potential.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.75\linewidth]{sections_us/images_us/fig_val_aei.pdf}
%     \caption{\textbf{Surface Index Validation Against Anthropic Economic Index}. State agreement between Anthropic Economic Index (rows) and Surface Index (columns) across three AI adoption categories. Strong agreement on leading states (8/13) and laggard states (9/13) validates the Surface Index methodology for identifying states with highest and lowest AI workforce exposure. Overall diagonal pattern shows 69\% perfect agreement across all categories.}
%     \label{fig:val_aei}
% \end{figure}

% The validation establishes that our capability assessment methodology produces geographically realistic patterns when compared to actual adoption behavior. However, validation is limited to current technology-sector adoption patterns among early adopters. The framework measures technical automation potential rather than predicting adoption rates, leaving timeline and implementation decisions to policymakers based on their specific policy priorities and local conditions. Forward adoption scenarios extend beyond available validation data and rely on assumptions about technology diffusion patterns derived from historical software adoption rather than AI-specific evidence. The validation provides empirical grounding for the framework's foundation while maintaining transparency about the speculative nature of long-term projections. The validation provides empirical grounding for the framework's foundation while maintaining transparency about the speculative nature of long-term projections.

\section*{Appendix B: Measuring Industry Concentration of AI Exposure}

While the Iceberg Index captures the overall level of AI-exposed work in a state, it does not reveal whether that exposure is concentrated in a few dominant industries or distributed broadly across the economy. This structural distinction has significant policy implications: states with concentrated exposure can focus on sector-specific strategies, while those with distributed exposure must prepare for system-wide transitions that span multiple sectors.

To quantify this structure, we apply a measure of industry concentration widely used in labor economics: the Herfindahl–Hirschman Index (HHI). Originally developed to assess market concentration, the HHI has been validated in labor-market research as a reliable tool for measuring employer dominance and sectoral risk~\cite{Azar2020LaborMarketConcentration, RevisitedHHI2023}. Here, we adapt it to capture how AI-related disruption is distributed across industries within each state.

The method proceeds as follows. For each state, we calculate the Iceberg Index contribution from each industry, reflecting the share of total exposed wage value attributable to that sector. We then express each industry’s contribution as a share of the state's total exposed wages, square these shares, and sum them to obtain a single concentration score. Formally, let $E_i$ be the exposed wage value for industry $i$, and let $E_{\text{total}} = \sum_i E_i$ denote the total exposed wage value in the state. We define the share of exposure from industry $i$ as $s_i = E_i / E_{\text{total}}$, and compute the HHI as:

\[
\text{HHI} = \sum_{i=1}^{N} s_i^2
\]

where $N$ is the number of industries. The HHI ranges from near zero (perfect diversification) to 1 (total concentration in one industry). For interpretability, we report values on a 0–10,000 scale, as is standard in economic literature. In our analysis, state-level HHI scores typically fall between 1400 and 1900.

To aid interpretation, we group states into three categories based on their HHI scores. Values below 1580 indicate distributed exposure across many sectors; values between 1580 and 1737 suggest moderately concentrated risk; and values above 1738 reflect states where most exposure is channeled into a few dominant industries. These thresholds loosely mirror antitrust classifications and align with the empirical distribution of concentration in our dataset.

This approach reveals clear regional patterns. States in the Northeast Corridor tend to show tightly clustered exposure, often concentrated in finance and technology. In contrast, states in the Manufacturing Belt exhibit more distributed profiles, with risk spread across production, logistics, administration, and related services. Smaller states vary widely: Iowa, for example, has one of the most diversified exposure profiles in the country (HHI = 1463), while Delaware concentrates a large share of its exposure in finance (HHI = 1741), despite having a comparable Iceberg Index.

Using a validated measure like the HHI ensures that structural comparisons across states are both rigorous and interpretable. It provides policymakers with a deeper view of AI disruption—not just how much exposure exists, but how that exposure is likely to manifest and propagate within each state’s economy.


\section*{Appendix C: Implementation Details}
The computational framework integrates two components to enable population-scale analysis.

\paragraph{(a) Large Population Models}: Implemented through the AgentTorch platform~\cite{chopra2024flame, chopra2025lpm}, providing the simulation engine for population-scale agent-based modeling. The platform supports efficient simulation of 151 million heterogeneous agents with distinct skill profiles while capturing interaction dynamics across geographic and occupational networks. LPMs have been validated through successful applications in epidemic response~\cite{chopra2023epidemiology}, biosecurity analysis~\cite{adiga2024biosecurity}, and supply chain resilience modeling~\cite{romero2021vaccine}, demonstrating their capability to handle complex population-scale dynamics. The framework incorporates behavioral adaptation mechanisms and technology adoption dynamics for realistic workforce response modeling. Core capabilities include scenario analysis through forward simulation, parameter estimation using real-world workforce data, and policy evaluation across multiple variables.

\paragraph{(b) High-Performance Computing Infrastructure}: Deployment on Oak Ridge National Laboratory's Frontier supercomputer~\cite{frontier2022}, one of the world's fastest computing systems, provides the computational execution layer. The infrastructure delivers massively parallel processing across thousands of compute nodes with GPU acceleration that supports large-scale optimization and analysis operations. This computational capacity transforms previously intractable population modeling challenges into feasible real-time policy analysis tools. These integrated components provide the computational foundation for the Iceberg Index and its policy scenario testing functionality, enabling dynamic analysis of workforce transformation scenarios under different policy conditions.